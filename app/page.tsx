'use client'

import { useState, useEffect, useRef, useCallback } from 'react'
import RobotAvatar from '@/components/RobotAvatar'
import ChatBubble from '@/components/ChatBubble'
import VoiceButton from '@/components/VoiceButton'
import StatusIndicator from '@/components/StatusIndicator'

type RobotState = 'idle' | 'listening' | 'thinking' | 'speaking'

interface Message {
  role: 'user' | 'assistant'
  content: string
}

export default function Home() {
  const [robotState, setRobotState] = useState<RobotState>('idle')
  const [messages, setMessages] = useState<Message[]>([])
  const [threadId, setThreadId] = useState<string | null>(null)
  const [micPermission, setMicPermission] = useState<'granted' | 'denied' | 'prompt'>('prompt')
  const [errorMessage, setErrorMessage] = useState<string | null>(null)
  const [recordingTime, setRecordingTime] = useState(0)
  const [audioUnlocked, setAudioUnlocked] = useState(false)
  
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const chatContainerRef = useRef<HTMLDivElement>(null)
  
  // éŒ„éŸ³ç›¸é—œ - ä½¿ç”¨ä¸€å€‹ ref ç‰©ä»¶ä¾†ç®¡ç†æ‰€æœ‰éŒ„éŸ³ç‹€æ…‹
  const recordingRef = useRef<{
    mediaRecorder: MediaRecorder | null
    stream: MediaStream | null
    audioContext: AudioContext | null
    analyser: AnalyserNode | null
    source: MediaStreamAudioSourceNode | null
    chunks: Blob[]
    timer: NodeJS.Timeout | null
    silenceTimer: NodeJS.Timeout | null
    lastSoundTime: number
    isRecording: boolean
  }>({
    mediaRecorder: null,
    stream: null,
    audioContext: null,
    analyser: null,
    source: null,
    chunks: [],
    timer: null,
    silenceTimer: null,
    lastSoundTime: Date.now(),
    isRecording: false
  })

  // è‡ªå‹•æ»¾å‹•
  useEffect(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight
    }
  }, [messages])

  // iOS éŸ³è¨Šè§£é–
  const unlockAudio = useCallback(() => {
    if (audioUnlocked) return
    
    if (audioRef.current) {
      audioRef.current.src = 'data:audio/mp3;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU4Ljc2LjEwMAAAAAAAAAAAAAAA/+M4wAAAAAAAAAAAAEluZm8AAAAPAAAAAwAAAbAAqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV//////////////////////////////////////////////////////////////////8AAAAATGF2YzU4LjEzAAAAAAAAAAAAAAAAJAAAAAAAAAAAAbD/OAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA='
      audioRef.current.play().then(() => {
        setAudioUnlocked(true)
      }).catch(() => {})
    }
  }, [audioUnlocked])

  // ç™¼é€è¨Šæ¯çµ¦ AI
  const sendToAssistant = useCallback(async (userMessage: string) => {
    setRobotState('thinking')
    setMessages(prev => [...prev, { role: 'user', content: userMessage }])

    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: userMessage, threadId })
      })

      if (!response.ok) throw new Error('Chat failed')

      const data = await response.json()
      
      if (data.threadId) {
        setThreadId(data.threadId)
      }

      const assistantMessage = data.message
      setMessages(prev => [...prev, { role: 'assistant', content: assistantMessage }])

      // æ’­æ”¾ TTS
      setRobotState('speaking')
      
      const ttsResponse = await fetch('/api/tts', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: assistantMessage })
      })

      if (ttsResponse.ok && audioRef.current) {
        const audioBlob = await ttsResponse.blob()
        const audioUrl = URL.createObjectURL(audioBlob)
        
        audioRef.current.src = audioUrl
        audioRef.current.onended = () => {
          setRobotState('idle')
          URL.revokeObjectURL(audioUrl)
        }
        audioRef.current.onerror = () => {
          setRobotState('idle')
        }
        
        try {
          await audioRef.current.play()
        } catch (e) {
          setErrorMessage('éŸ³è¨Šæ’­æ”¾å¤±æ•—ï¼Œè«‹é»æ“Šç•«é¢å¾Œå†è©¦')
          setRobotState('idle')
        }
      } else {
        setRobotState('idle')
      }

    } catch (error) {
      console.error('Chat error:', error)
      setMessages(prev => [...prev, { role: 'assistant', content: 'æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›å•é¡Œï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚' }])
      setRobotState('idle')
    }
  }, [threadId])

  // ä½¿ç”¨ Whisper API è½‰éŒ„éŸ³è¨Š
  const transcribeAudio = useCallback(async (audioBlob: Blob) => {
    setRobotState('thinking')
    
    try {
      const formData = new FormData()
      formData.append('audio', audioBlob, 'recording.webm')

      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        throw new Error('Transcription failed')
      }

      const data = await response.json()
      const text = data.text?.trim()

      if (text) {
        await sendToAssistant(text)
      } else {
        setErrorMessage('æ²’æœ‰è­˜åˆ¥åˆ°èªéŸ³å…§å®¹ï¼Œè«‹å†è©¦ä¸€æ¬¡')
        setRobotState('idle')
      }

    } catch (error) {
      console.error('Transcription error:', error)
      setErrorMessage('èªéŸ³è­˜åˆ¥å¤±æ•—ï¼Œè«‹å†è©¦ä¸€æ¬¡')
      setRobotState('idle')
    }
  }, [sendToAssistant])

  // å®Œå…¨æ¸…ç†éŒ„éŸ³è³‡æº
  const cleanupRecording = useCallback(() => {
    const rec = recordingRef.current
    
    // æ¸…ç†å®šæ™‚å™¨
    if (rec.timer) {
      clearInterval(rec.timer)
      rec.timer = null
    }
    if (rec.silenceTimer) {
      clearInterval(rec.silenceTimer)
      rec.silenceTimer = null
    }
    
    // æ–·é–‹éŸ³è¨Šç¯€é»
    if (rec.source) {
      try { rec.source.disconnect() } catch (e) {}
      rec.source = null
    }
    rec.analyser = null
    
    // é—œé–‰ AudioContext
    if (rec.audioContext && rec.audioContext.state !== 'closed') {
      try { rec.audioContext.close() } catch (e) {}
      rec.audioContext = null
    }
    
    // åœæ­¢ MediaStream
    if (rec.stream) {
      rec.stream.getTracks().forEach(track => {
        track.stop()
      })
      rec.stream = null
    }
    
    rec.mediaRecorder = null
    rec.chunks = []
    rec.isRecording = false
  }, [])

  // åœæ­¢éŒ„éŸ³
  const stopListening = useCallback(() => {
    const rec = recordingRef.current
    
    if (rec.timer) {
      clearInterval(rec.timer)
      rec.timer = null
    }
    if (rec.silenceTimer) {
      clearInterval(rec.silenceTimer)
      rec.silenceTimer = null
    }
    
    if (rec.mediaRecorder && rec.mediaRecorder.state === 'recording') {
      rec.isRecording = false
      rec.mediaRecorder.stop()
    } else {
      // å¦‚æœæ²’æœ‰åœ¨éŒ„éŸ³ï¼Œç›´æ¥é‡ç½®ç‹€æ…‹
      cleanupRecording()
      setRobotState('idle')
    }
  }, [cleanupRecording])

  // é–‹å§‹éŒ„éŸ³
  const startListening = useCallback(async () => {
    // å…ˆå®Œå…¨æ¸…ç†ä¹‹å‰çš„éŒ„éŸ³
    cleanupRecording()
    
    setErrorMessage(null)
    unlockAudio()

    const rec = recordingRef.current

    try {
      // ç²å–æ–°çš„ MediaStream
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
        } 
      })
      
      rec.stream = stream
      setMicPermission('granted')
      
      // é‡ç½®éŒ„éŸ³æ•¸æ“š
      rec.chunks = []
      rec.lastSoundTime = Date.now()
      setRecordingTime(0)

      // å‰µå»ºæ–°çš„ AudioContext
      const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext
      rec.audioContext = new AudioContextClass()
      
      // ç¢ºä¿ AudioContext åœ¨é‹è¡Œ
      if (rec.audioContext.state === 'suspended') {
        await rec.audioContext.resume()
      }
      
      // å‰µå»ºéŸ³è¨Šåˆ†æå™¨
      rec.source = rec.audioContext.createMediaStreamSource(stream)
      rec.analyser = rec.audioContext.createAnalyser()
      rec.analyser.fftSize = 256
      rec.source.connect(rec.analyser)

      // æ±ºå®šæ”¯æ´çš„æ ¼å¼
      let mimeType = 'audio/webm'
      if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
        mimeType = 'audio/webm;codecs=opus'
      } else if (MediaRecorder.isTypeSupported('audio/mp4')) {
        mimeType = 'audio/mp4'
      }

      const mediaRecorder = new MediaRecorder(stream, { mimeType })
      rec.mediaRecorder = mediaRecorder

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          rec.chunks.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        const chunks = [...rec.chunks] // è¤‡è£½ä¸€ä»½
        const currentMimeType = mimeType
        
        // ç«‹å³æ¸…ç†è³‡æº
        cleanupRecording()

        if (chunks.length > 0) {
          const audioBlob = new Blob(chunks, { type: currentMimeType })
          
          if (audioBlob.size < 1000) {
            setErrorMessage('éŒ„éŸ³æ™‚é–“å¤ªçŸ­ï¼Œè«‹èªªé•·ä¸€é»')
            setRobotState('idle')
            return
          }

          await transcribeAudio(audioBlob)
        } else {
          setRobotState('idle')
        }
      }

      mediaRecorder.onerror = () => {
        cleanupRecording()
        setErrorMessage('éŒ„éŸ³ç™¼ç”ŸéŒ¯èª¤')
        setRobotState('idle')
      }

      // é–‹å§‹éŒ„éŸ³
      mediaRecorder.start(100)
      rec.isRecording = true
      setRobotState('listening')

      // è¨ˆæ™‚å™¨
      rec.timer = setInterval(() => {
        setRecordingTime(prev => {
          if (prev >= 30) {
            stopListening()
            return prev
          }
          return prev + 1
        })
      }, 1000)

      // éœéŸ³æª¢æ¸¬
      rec.silenceTimer = setInterval(() => {
        if (!rec.analyser || !rec.isRecording) return

        const dataArray = new Uint8Array(rec.analyser.frequencyBinCount)
        rec.analyser.getByteFrequencyData(dataArray)
        
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length
        
        if (average > 10) {
          rec.lastSoundTime = Date.now()
        } else {
          const silenceDuration = Date.now() - rec.lastSoundTime
          if (silenceDuration > 1500 && rec.isRecording) {
            stopListening()
          }
        }
      }, 100)

    } catch (error) {
      console.error('Failed to start recording:', error)
      cleanupRecording()
      
      if ((error as Error).name === 'NotAllowedError') {
        setMicPermission('denied')
        setErrorMessage('è«‹å…è¨±éº¥å…‹é¢¨æ¬Šé™')
      } else {
        setErrorMessage('ç„¡æ³•å•Ÿå‹•éŒ„éŸ³: ' + (error as Error).message)
      }
      setRobotState('idle')
    }
  }, [unlockAudio, transcribeAudio, stopListening, cleanupRecording])

  // è™•ç†æŒ‰éˆ•é»æ“Š
  const handleVoiceButtonPress = useCallback(() => {
    unlockAudio()
    
    if (robotState === 'listening') {
      stopListening()
    } else if (robotState === 'idle') {
      startListening()
    }
  }, [robotState, startListening, stopListening, unlockAudio])

  // é»æ“Šé é¢è§£é–éŸ³è¨Š
  const handlePageClick = useCallback(() => {
    unlockAudio()
  }, [unlockAudio])

  // è«‹æ±‚æ¬Šé™æŒ‰éˆ•
  const handleRequestPermission = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      stream.getTracks().forEach(track => track.stop())
      setMicPermission('granted')
      unlockAudio()
    } catch (e) {
      setMicPermission('denied')
      setErrorMessage('éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•')
    }
  }, [unlockAudio])

  // çµ„ä»¶å¸è¼‰æ™‚æ¸…ç†
  useEffect(() => {
    return () => {
      cleanupRecording()
    }
  }, [cleanupRecording])

  return (
    <main className="h-screen flex flex-col overflow-hidden" onClick={handlePageClick}>
      <audio ref={audioRef} playsInline preload="auto" />

      {/* é ‚éƒ¨ç‹€æ…‹åˆ— */}
      <header className="flex-none px-4 py-3 flex items-center justify-between bg-black/20 backdrop-blur-sm">
        <div className="flex items-center gap-2">
          <StatusIndicator state={robotState} />
          <span className="text-sm text-gray-300">
            {robotState === 'idle' && 'å¾…æ©Ÿä¸­'}
            {robotState === 'listening' && `éŒ„éŸ³ä¸­ ${recordingTime}s`}
            {robotState === 'thinking' && 'è™•ç†ä¸­...'}
            {robotState === 'speaking' && 'å›è¦†ä¸­...'}
          </span>
        </div>
        
        <div className="flex items-center gap-2">
          {micPermission === 'granted' && (
            <span className="text-xs text-green-400">ğŸ¤</span>
          )}
          {audioUnlocked && (
            <span className="text-xs text-blue-400">ğŸ”Š</span>
          )}
        </div>
      </header>

      {/* éŒ¯èª¤è¨Šæ¯ */}
      {errorMessage && (
        <div className="mx-4 mt-2 p-3 bg-red-500/20 border border-red-500/30 rounded-lg text-red-300 text-sm text-center">
          {errorMessage}
          <button 
            onClick={() => setErrorMessage(null)}
            className="ml-2 text-red-400 hover:text-red-300"
          >
            âœ•
          </button>
        </div>
      )}

      {/* æ©Ÿå™¨äººå‹•ç•«å€ */}
      <section className="flex-none h-[32vh] flex items-center justify-center">
        <RobotAvatar state={robotState} />
      </section>

      {/* å°è©±å€ */}
      <section 
        ref={chatContainerRef}
        className="flex-1 overflow-y-auto px-4 pb-4 space-y-3"
      >
        {messages.length === 0 && (
          <div className="text-center text-gray-400 mt-4">
            <p className="text-lg mb-2">ğŸ‘‹ ä½ å¥½ï¼æˆ‘æ˜¯æ™ºæ…§å®¢æœåŠ©ç†</p>
            <p className="text-sm mb-4">é»æ“Šéº¥å…‹é¢¨æŒ‰éˆ•é–‹å§‹èªªè©±ï¼Œèªªå®Œæœƒè‡ªå‹•åœæ­¢</p>
            
            {micPermission === 'prompt' && (
              <button
                onClick={handleRequestPermission}
                className="px-4 py-2 bg-robot-blue/20 text-robot-blue rounded-full text-sm hover:bg-robot-blue/30 transition-colors"
              >
                ğŸ¤ é»æ“Šæˆæ¬Šéº¥å…‹é¢¨
              </button>
            )}
          </div>
        )}
        
        {messages.map((msg, index) => (
          <ChatBubble 
            key={index} 
            role={msg.role} 
            content={msg.content} 
          />
        ))}

        {/* æ€è€ƒä¸­ */}
        {robotState === 'thinking' && (
          <div className="flex justify-center gap-1 py-2">
            <span className="thinking-dot w-2 h-2 bg-robot-blue rounded-full"></span>
            <span className="thinking-dot w-2 h-2 bg-robot-blue rounded-full"></span>
            <span className="thinking-dot w-2 h-2 bg-robot-blue rounded-full"></span>
          </div>
        )}
      </section>

      {/* åº•éƒ¨æ§åˆ¶å€ */}
      <footer className="flex-none px-4 py-6 bg-gradient-to-t from-black/40 to-transparent">
        <div className="flex items-center justify-center gap-4">
          <VoiceButton 
            state={robotState}
            onPress={handleVoiceButtonPress}
            onRelease={() => {}}
            disabled={robotState === 'thinking' || robotState === 'speaking'}
          />
        </div>
        
        <p className="text-center text-xs text-gray-500 mt-4">
          {robotState === 'listening' 
            ? 'èªªå®Œå¾Œæœƒè‡ªå‹•åœæ­¢' 
            : micPermission === 'denied' 
            ? 'è«‹å…ˆæˆæ¬Šéº¥å…‹é¢¨æ¬Šé™' 
            : 'é»æ“ŠæŒ‰éˆ•é–‹å§‹èªªè©±'}
        </p>
      </footer>
    </main>
  )
}
