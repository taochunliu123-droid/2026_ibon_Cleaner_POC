'use client'

import { useState, useEffect, useRef, useCallback } from 'react'
import RobotAvatar from '@/components/RobotAvatar'
import ChatBubble from '@/components/ChatBubble'
import VoiceButton from '@/components/VoiceButton'
import StatusIndicator from '@/components/StatusIndicator'

type RobotState = 'idle' | 'listening' | 'thinking' | 'speaking'

interface Message {
  role: 'user' | 'assistant'
  content: string
}

export default function Home() {
  const [robotState, setRobotState] = useState<RobotState>('idle')
  const [messages, setMessages] = useState<Message[]>([])
  const [threadId, setThreadId] = useState<string | null>(null)
  const [micPermission, setMicPermission] = useState<'granted' | 'denied' | 'prompt'>('prompt')
  const [errorMessage, setErrorMessage] = useState<string | null>(null)
  const [recordingTime, setRecordingTime] = useState(0)
  const [audioUnlocked, setAudioUnlocked] = useState(false)
  
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const chatContainerRef = useRef<HTMLDivElement>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const streamRef = useRef<MediaStream | null>(null)
  const timerRef = useRef<NodeJS.Timeout | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const lastSoundTimeRef = useRef<number>(Date.now())
  
  const robotStateRef = useRef<RobotState>('idle')
  
  useEffect(() => {
    robotStateRef.current = robotState
  }, [robotState])

  // è‡ªå‹•æ»¾å‹•
  useEffect(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight
    }
  }, [messages])

  // æ¸…ç†
  useEffect(() => {
    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop())
      }
      if (timerRef.current) clearInterval(timerRef.current)
      if (silenceTimerRef.current) clearInterval(silenceTimerRef.current)
      if (audioContextRef.current) audioContextRef.current.close()
    }
  }, [])

  // iOS éŸ³è¨Šè§£é– - å¿…é ˆåœ¨ç”¨æˆ¶äº’å‹•æ™‚èª¿ç”¨
  const unlockAudio = useCallback(() => {
    if (audioUnlocked) return
    
    if (audioRef.current) {
      // æ’­æ”¾ä¸€å€‹éœéŸ³ä¾†è§£é– iOS éŸ³è¨Š
      audioRef.current.src = 'data:audio/mp3;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU4Ljc2LjEwMAAAAAAAAAAAAAAA/+M4wAAAAAAAAAAAAEluZm8AAAAPAAAAAwAAAbAAqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV//////////////////////////////////////////////////////////////////8AAAAATGF2YzU4LjEzAAAAAAAAAAAAAAAAJAAAAAAAAAAAAbD/OAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA='
      audioRef.current.play().then(() => {
        setAudioUnlocked(true)
        console.log('Audio unlocked for iOS')
      }).catch(e => console.log('Audio unlock failed:', e))
    }
  }, [audioUnlocked])

  // è«‹æ±‚éº¥å…‹é¢¨æ¬Šé™
  const requestMicPermission = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000,
        } 
      })
      streamRef.current = stream
      setMicPermission('granted')
      setErrorMessage(null)
      
      // åŒæ™‚è§£é–éŸ³è¨Š
      unlockAudio()
      
      return stream
    } catch (error) {
      console.error('Microphone permission error:', error)
      setMicPermission('denied')
      setErrorMessage('è«‹å…è¨±éº¥å…‹é¢¨æ¬Šé™æ‰èƒ½ä½¿ç”¨èªéŸ³åŠŸèƒ½')
      return null
    }
  }, [unlockAudio])

  // ç™¼é€è¨Šæ¯çµ¦ AI
  const sendToAssistant = useCallback(async (userMessage: string) => {
    setRobotState('thinking')
    setMessages(prev => [...prev, { role: 'user', content: userMessage }])

    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: userMessage, threadId })
      })

      if (!response.ok) throw new Error('Chat failed')

      const data = await response.json()
      
      if (data.threadId) {
        setThreadId(data.threadId)
      }

      const assistantMessage = data.message
      setMessages(prev => [...prev, { role: 'assistant', content: assistantMessage }])

      // æ’­æ”¾ TTS
      setRobotState('speaking')
      
      const ttsResponse = await fetch('/api/tts', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: assistantMessage })
      })

      if (ttsResponse.ok && audioRef.current) {
        const audioBlob = await ttsResponse.blob()
        const audioUrl = URL.createObjectURL(audioBlob)
        
        audioRef.current.src = audioUrl
        audioRef.current.onended = () => {
          setRobotState('idle')
          URL.revokeObjectURL(audioUrl)
        }
        audioRef.current.onerror = () => {
          console.error('Audio playback error')
          setRobotState('idle')
        }
        
        try {
          await audioRef.current.play()
        } catch (e) {
          console.error('Play failed:', e)
          setErrorMessage('éŸ³è¨Šæ’­æ”¾å¤±æ•—ï¼Œè«‹é»æ“Šç•«é¢ä»»æ„è™•å¾Œå†è©¦')
          setRobotState('idle')
        }
      } else {
        setRobotState('idle')
      }

    } catch (error) {
      console.error('Chat error:', error)
      setMessages(prev => [...prev, { role: 'assistant', content: 'æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›å•é¡Œï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚' }])
      setRobotState('idle')
    }
  }, [threadId])

  // ä½¿ç”¨ Whisper API è½‰éŒ„éŸ³è¨Š
  const transcribeAudio = useCallback(async (audioBlob: Blob) => {
    setRobotState('thinking')
    
    try {
      const formData = new FormData()
      formData.append('audio', audioBlob, 'recording.webm')

      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        throw new Error('Transcription failed')
      }

      const data = await response.json()
      const text = data.text?.trim()

      if (text) {
        await sendToAssistant(text)
      } else {
        setErrorMessage('æ²’æœ‰è­˜åˆ¥åˆ°èªéŸ³å…§å®¹ï¼Œè«‹å†è©¦ä¸€æ¬¡')
        setRobotState('idle')
      }

    } catch (error) {
      console.error('Transcription error:', error)
      setErrorMessage('èªéŸ³è­˜åˆ¥å¤±æ•—ï¼Œè«‹å†è©¦ä¸€æ¬¡')
      setRobotState('idle')
    }
  }, [sendToAssistant])

  // æª¢æ¸¬éœéŸ³ - ç”¨æ–¼è‡ªå‹•åœæ­¢éŒ„éŸ³
  const checkSilence = useCallback(() => {
    if (!analyserRef.current) return

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount)
    analyserRef.current.getByteFrequencyData(dataArray)
    
    // è¨ˆç®—éŸ³é‡
    const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length
    
    if (average > 10) {
      // æœ‰è²éŸ³
      lastSoundTimeRef.current = Date.now()
    } else {
      // éœéŸ³ - æª¢æŸ¥æ˜¯å¦è¶…é 1.5 ç§’
      const silenceDuration = Date.now() - lastSoundTimeRef.current
      if (silenceDuration > 1500 && robotStateRef.current === 'listening') {
        // è‡ªå‹•åœæ­¢éŒ„éŸ³
        stopListening()
      }
    }
  }, [])

  // åœæ­¢éŒ„éŸ³
  const stopListening = useCallback(() => {
    if (silenceTimerRef.current) {
      clearInterval(silenceTimerRef.current)
      silenceTimerRef.current = null
    }
    if (timerRef.current) {
      clearInterval(timerRef.current)
      timerRef.current = null
    }
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
    }
  }, [])

  // é–‹å§‹éŒ„éŸ³
  const startListening = useCallback(async () => {
    if (robotStateRef.current !== 'idle') return

    setErrorMessage(null)
    
    // è§£é–éŸ³è¨Š
    unlockAudio()

    // å–å¾—æˆ–è«‹æ±‚éº¥å…‹é¢¨æ¬Šé™
    let stream = streamRef.current
    if (!stream || !stream.active) {
      stream = await requestMicPermission()
      if (!stream) return
    }

    try {
      audioChunksRef.current = []
      setRecordingTime(0)
      lastSoundTimeRef.current = Date.now()

      // è¨­ç½®éŸ³è¨Šåˆ†æå™¨ç”¨æ–¼éœéŸ³æª¢æ¸¬
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)()
      }
      
      const source = audioContextRef.current.createMediaStreamSource(stream)
      analyserRef.current = audioContextRef.current.createAnalyser()
      analyserRef.current.fftSize = 256
      source.connect(analyserRef.current)

      // æ±ºå®šæ”¯æ´çš„æ ¼å¼
      let mimeType = 'audio/webm'
      if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
        mimeType = 'audio/webm;codecs=opus'
      } else if (MediaRecorder.isTypeSupported('audio/mp4')) {
        mimeType = 'audio/mp4'
      }

      const mediaRecorder = new MediaRecorder(stream, { mimeType })
      mediaRecorderRef.current = mediaRecorder

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        if (silenceTimerRef.current) {
          clearInterval(silenceTimerRef.current)
          silenceTimerRef.current = null
        }
        if (timerRef.current) {
          clearInterval(timerRef.current)
          timerRef.current = null
        }

        if (audioChunksRef.current.length > 0) {
          const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })
          
          if (audioBlob.size < 1000) {
            setErrorMessage('éŒ„éŸ³æ™‚é–“å¤ªçŸ­ï¼Œè«‹èªªé•·ä¸€é»')
            setRobotState('idle')
            return
          }

          await transcribeAudio(audioBlob)
        } else {
          setRobotState('idle')
        }
      }

      mediaRecorder.onerror = () => {
        setErrorMessage('éŒ„éŸ³ç™¼ç”ŸéŒ¯èª¤')
        setRobotState('idle')
      }

      // é–‹å§‹éŒ„éŸ³
      mediaRecorder.start(100)
      setRobotState('listening')

      // è¨ˆæ™‚å™¨
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => {
          if (prev >= 30) {
            stopListening()
            return prev
          }
          return prev + 1
        })
      }, 1000)

      // éœéŸ³æª¢æ¸¬ - æ¯ 100ms æª¢æŸ¥ä¸€æ¬¡
      silenceTimerRef.current = setInterval(checkSilence, 100)

    } catch (error) {
      console.error('Failed to start recording:', error)
      setErrorMessage('ç„¡æ³•å•Ÿå‹•éŒ„éŸ³')
      setRobotState('idle')
    }
  }, [requestMicPermission, transcribeAudio, checkSilence, stopListening, unlockAudio])

  // è™•ç†æŒ‰éˆ•é»æ“Š
  const handleVoiceButtonPress = useCallback(() => {
    // æ¯æ¬¡é»æ“Šéƒ½å…ˆè§£é–éŸ³è¨Š
    unlockAudio()
    
    if (robotStateRef.current === 'listening') {
      stopListening()
    } else if (robotStateRef.current === 'idle') {
      startListening()
    }
  }, [startListening, stopListening, unlockAudio])

  // é»æ“Šé é¢è§£é–éŸ³è¨Š
  const handlePageClick = useCallback(() => {
    unlockAudio()
  }, [unlockAudio])

  return (
    <main className="h-screen flex flex-col overflow-hidden" onClick={handlePageClick}>
      <audio ref={audioRef} playsInline preload="auto" />

      {/* é ‚éƒ¨ç‹€æ…‹åˆ— */}
      <header className="flex-none px-4 py-3 flex items-center justify-between bg-black/20 backdrop-blur-sm">
        <div className="flex items-center gap-2">
          <StatusIndicator state={robotState} />
          <span className="text-sm text-gray-300">
            {robotState === 'idle' && 'å¾…æ©Ÿä¸­'}
            {robotState === 'listening' && `éŒ„éŸ³ä¸­ ${recordingTime}s`}
            {robotState === 'thinking' && 'è™•ç†ä¸­...'}
            {robotState === 'speaking' && 'å›è¦†ä¸­...'}
          </span>
        </div>
        
        <div className="flex items-center gap-2">
          {micPermission === 'granted' && (
            <span className="text-xs text-green-400">ğŸ¤ å·²æˆæ¬Š</span>
          )}
          {audioUnlocked && (
            <span className="text-xs text-blue-400">ğŸ”Š</span>
          )}
        </div>
      </header>

      {/* éŒ¯èª¤è¨Šæ¯ */}
      {errorMessage && (
        <div className="mx-4 mt-2 p-3 bg-red-500/20 border border-red-500/30 rounded-lg text-red-300 text-sm text-center">
          {errorMessage}
          <button 
            onClick={() => setErrorMessage(null)}
            className="ml-2 text-red-400 hover:text-red-300"
          >
            âœ•
          </button>
        </div>
      )}

      {/* æ©Ÿå™¨äººå‹•ç•«å€ */}
      <section className="flex-none h-[32vh] flex items-center justify-center">
        <RobotAvatar state={robotState} />
      </section>

      {/* å°è©±å€ */}
      <section 
        ref={chatContainerRef}
        className="flex-1 overflow-y-auto px-4 pb-4 space-y-3"
      >
        {messages.length === 0 && (
          <div className="text-center text-gray-400 mt-4">
            <p className="text-lg mb-2">ğŸ‘‹ ä½ å¥½ï¼æˆ‘æ˜¯æ™ºæ…§å®¢æœåŠ©ç†</p>
            <p className="text-sm mb-4">é»æ“Šéº¥å…‹é¢¨æŒ‰éˆ•é–‹å§‹èªªè©±ï¼Œèªªå®Œæœƒè‡ªå‹•åœæ­¢</p>
            
            {micPermission === 'prompt' && (
              <button
                onClick={requestMicPermission}
                className="px-4 py-2 bg-robot-blue/20 text-robot-blue rounded-full text-sm hover:bg-robot-blue/30 transition-colors"
              >
                ğŸ¤ é»æ“Šæˆæ¬Šéº¥å…‹é¢¨
              </button>
            )}
          </div>
        )}
        
        {messages.map((msg, index) => (
          <ChatBubble 
            key={index} 
            role={msg.role} 
            content={msg.content} 
          />
        ))}

        {/* æ€è€ƒä¸­ */}
        {robotState === 'thinking' && (
          <div className="flex justify-center gap-1 py-2">
            <span className="thinking-dot w-2 h-2 bg-robot-blue rounded-full"></span>
            <span className="thinking-dot w-2 h-2 bg-robot-blue rounded-full"></span>
            <span className="thinking-dot w-2 h-2 bg-robot-blue rounded-full"></span>
          </div>
        )}
      </section>

      {/* åº•éƒ¨æ§åˆ¶å€ */}
      <footer className="flex-none px-4 py-6 bg-gradient-to-t from-black/40 to-transparent">
        <div className="flex items-center justify-center gap-4">
          <VoiceButton 
            state={robotState}
            onPress={handleVoiceButtonPress}
            onRelease={() => {}}
            disabled={robotState === 'thinking' || robotState === 'speaking'}
          />
        </div>
        
        <p className="text-center text-xs text-gray-500 mt-4">
          {robotState === 'listening' 
            ? 'èªªå®Œå¾Œæœƒè‡ªå‹•åœæ­¢ï¼Œæˆ–é»æ“ŠæŒ‰éˆ•æ‰‹å‹•åœæ­¢' 
            : micPermission === 'denied' 
            ? 'è«‹å…ˆæˆæ¬Šéº¥å…‹é¢¨æ¬Šé™' 
            : 'é»æ“ŠæŒ‰éˆ•é–‹å§‹èªªè©±'}
        </p>
      </footer>
    </main>
  )
}
